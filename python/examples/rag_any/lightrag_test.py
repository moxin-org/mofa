#!/usr/bin/env python3
"""
LightRAG æµ‹è¯•ç‰ˆæœ¬ - ä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥å‘é‡æ¥æµ‹è¯•ç³»ç»Ÿ
å½“åµŒå…¥æœåŠ¡æœ‰é—®é¢˜æ—¶çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
"""

import os
import asyncio
import logging
import numpy as np
from pathlib import Path

# å¯¼å…¥ LightRAG æ ¸å¿ƒç»„ä»¶
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

# é¿å…ä»£ç†é—®é¢˜
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

# é…ç½®
OLLAMA_HOST = "10.100.1.115:11434"
LLM_MODEL = "gpt-oss:20b"
EMBED_MODEL = "dengcao/Qwen3-Embedding-8B:Q5_K_M"


async def ollama_llm_func(prompt, system_prompt=None, history_messages=None, **kwargs):
    """Ollama LLM å‡½æ•°"""
    import httpx
    
    try:
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"http://{OLLAMA_HOST}/api/generate",
                json={
                    "model": LLM_MODEL,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 1000,
                        "temperature": 0.7
                    }
                }
            )
            if response.status_code == 200:
                return response.json().get("response", "No response")
            else:
                return f"Error: {response.status_code}"
    except Exception as e:
        logging.error(f"LLM è¯·æ±‚å¤±è´¥: {e}")
        return f"LLM Error: {str(e)}"


async def mock_embed_func(texts, **kwargs):
    """æ¨¡æ‹ŸåµŒå…¥å‡½æ•° - ç”Ÿæˆéšæœºä½†ä¸€è‡´çš„åµŒå…¥å‘é‡"""
    import hashlib
    
    if isinstance(texts, str):
        texts = [texts]
    
    results = []
    
    for text in texts:
        # åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆä¸€è‡´çš„ç§å­
        seed = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        np.random.seed(seed)
        
        # ç”Ÿæˆ8192ç»´çš„éšæœºåµŒå…¥å‘é‡
        embedding = np.random.normal(0, 1, 8192).astype(np.float32)
        # å½’ä¸€åŒ–å‘é‡
        embedding = embedding / np.linalg.norm(embedding)
        results.append(embedding.tolist())
    
    logging.info(f"âœ… ç”Ÿæˆäº† {len(results)} ä¸ªæ¨¡æ‹ŸåµŒå…¥å‘é‡")
    return results


async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸš€ LightRAG æµ‹è¯•ç‰ˆæœ¬ (ä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥)")
    print("=" * 50)
    
    try:
        # åˆ›å»ºåµŒå…¥å‡½æ•°
        embedding_func = EmbeddingFunc(
            embedding_dim=8192,  # Qwen3-Embedding-8B çš„ç»´åº¦
            max_token_size=8192,
            func=mock_embed_func  # ä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥å‡½æ•°
        )
        
        # åˆ›å»º LightRAG å®ä¾‹
        rag = LightRAG(
            working_dir="./lightrag_test_storage",
            llm_model_func=ollama_llm_func,
            embedding_func=embedding_func,
            enable_llm_cache=True
        )
        
        # åˆå§‹åŒ–å­˜å‚¨
        await rag.initialize_storages()
        await initialize_pipeline_status()
        
        print("âœ… LightRAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # æ’å…¥ä¸€äº›æµ‹è¯•æ•°æ®
        test_docs = [
            "ä¸ªäººæœåŠ¡å™¨é©±åŠ¨å®‰è£…ï¼šNVIDIAé©±åŠ¨å®‰è£…æ­¥éª¤åŒ…æ‹¬ä¸‹è½½æœ€æ–°é©±åŠ¨ã€å¸è½½æ—§é©±åŠ¨ã€å®‰è£…æ–°é©±åŠ¨å¹¶é‡å¯ç³»ç»Ÿã€‚",
            "MoFAæ˜¯ä¸€ä¸ªåŸºäºDORA-RSçš„æ¨¡å—åŒ–AIä»£ç†æ¡†æ¶ï¼Œæ”¯æŒé€šè¿‡dataflowé…ç½®è¿æ¥ä¸åŒçš„èŠ‚ç‚¹å’Œä»£ç†ã€‚",
            "Pythonè™šæ‹Ÿç¯å¢ƒç®¡ç†å¯ä»¥ä½¿ç”¨condaã€venvæˆ–poetryç­‰å·¥å…·ï¼Œæ¨èä½¿ç”¨condaè¿›è¡Œç¯å¢ƒéš”ç¦»ã€‚"
        ]
        
        print("ğŸ“ æ’å…¥æµ‹è¯•æ–‡æ¡£...")
        for i, doc in enumerate(test_docs, 1):
            try:
                await rag.ainsert(doc)
                print(f"âœ… æ’å…¥æ–‡æ¡£ {i}: {doc[:50]}...")
            except Exception as e:
                print(f"âŒ æ’å…¥æ–‡æ¡£ {i} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # äº¤äº’å¼æŸ¥è¯¢
        print("\nğŸ‰ ç³»ç»Ÿå°±ç»ªï¼å¼€å§‹äº¤äº’å¼é—®ç­”")
        print("ğŸ’¡ è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        print("âš ï¸  æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥å‘é‡ï¼ŒæŸ¥è¯¢ç»“æœå¯èƒ½ä¸å‡†ç¡®")
        
        while True:
            try:
                question = input("\nğŸ—£ï¸  è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if not question:
                    continue
                    
                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                print(f"ğŸ” æŸ¥è¯¢ä¸­...")
                
                # æ‰§è¡ŒæŸ¥è¯¢
                try:
                    response = await rag.aquery(question)
                    print("=" * 60)
                    print("ğŸ¯ æŸ¥è¯¢ç»“æœ")
                    print("=" * 60)
                    print(f"â“ é—®é¢˜: {question}")
                    print(f"ğŸ¤– å›ç­”: {response}")
                    print("=" * 60)
                except Exception as e:
                    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†å‡ºé”™: {e}")
    
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        logging.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())