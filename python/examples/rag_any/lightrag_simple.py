#!/usr/bin/env python3
"""
ç®€åŒ–çš„ LightRAG äº¤äº’å¼é—®ç­”ç³»ç»Ÿ
ä¸“æ³¨äºæŸ¥è¯¢åŠŸèƒ½ï¼Œé¿å…å¤æ‚çš„æ‰¹é‡æ–‡æ¡£å¤„ç†
"""

import os
import asyncio
import logging
from pathlib import Path

# å¯¼å…¥ LightRAG æ ¸å¿ƒç»„ä»¶
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

# é¿å…ä»£ç†é—®é¢˜
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

# é…ç½®
OLLAMA_HOST = "10.100.1.115:11434"
LLM_MODEL = "gpt-oss:20b"
EMBED_MODEL = "dengcao/Qwen3-Embedding-8B:Q5_K_M"


async def ollama_llm_func(prompt, system_prompt=None, history_messages=None, **kwargs):
    """Ollama LLM å‡½æ•°"""
    import httpx
    
    try:
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"http://{OLLAMA_HOST}/api/generate",
                json={
                    "model": LLM_MODEL,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 1000,
                        "temperature": 0.7
                    }
                }
            )
            if response.status_code == 200:
                return response.json().get("response", "No response")
            else:
                return f"Error: {response.status_code}"
    except Exception as e:
        logging.error(f"LLM è¯·æ±‚å¤±è´¥: {e}")
        return f"LLM Error: {str(e)}"


async def ollama_embed_func(texts, **kwargs):
    """Ollama åµŒå…¥å‡½æ•°"""
    import httpx
    import numpy as np
    
    if isinstance(texts, str):
        texts = [texts]
    
    results = []
    
    # éœ€è¦ä½¿ç”¨ä¸åŒçš„ä¸»æœºç«¯å£ç”¨äºåµŒå…¥æœåŠ¡
    embed_host = "10.100.1.115:11434"  # ä½¿ç”¨ä¸“é—¨çš„åµŒå…¥æœåŠ¡ç«¯å£
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        for text in texts:
            try:
                response = await client.post(
                    f"http://{embed_host}/api/embed",  # æ­£ç¡®çš„embedç«¯ç‚¹
                    json={
                        "model": EMBED_MODEL,
                        "input": text  # æ­£ç¡®çš„inputå‚æ•°
                    }
                )
                if response.status_code == 200:
                    embedding = response.json().get("embedding", [])
                    if embedding and len(embedding) > 0:
                        results.append(embedding)
                    else:
                        logging.error(f"âŒ ç©ºçš„ embedding å“åº”")
                        return None
                else:
                    logging.error(f"âŒ Embedding API é”™è¯¯: {response.status_code}")
                    return None
            except Exception as e:
                logging.error(f"âŒ åµŒå…¥è¯·æ±‚å¤±è´¥: {e}")
                return None
    
    return results


async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸš€ ç®€åŒ–çš„ LightRAG é—®ç­”ç³»ç»Ÿ")
    print("=" * 50)
    
    try:
        # åˆ›å»ºåµŒå…¥å‡½æ•°
        embedding_func = EmbeddingFunc(
            embedding_dim=8192,  # Qwen3-Embedding-8B çš„ç»´åº¦
            max_token_size=8192,
            func=ollama_embed_func
        )
        
        # åˆ›å»º LightRAG å®ä¾‹
        rag = LightRAG(
            working_dir="./lightrag_simple_storage",
            llm_model_func=ollama_llm_func,
            embedding_func=embedding_func,
            enable_llm_cache=True
        )
        
        # åˆå§‹åŒ–å­˜å‚¨
        await rag.initialize_storages()
        await initialize_pipeline_status()
        
        print("âœ… LightRAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # æ’å…¥ä¸€äº›æµ‹è¯•æ•°æ®
        test_docs = [
            "ä¸ªäººæœåŠ¡å™¨é©±åŠ¨å®‰è£…ï¼šNVIDIAé©±åŠ¨å®‰è£…æ­¥éª¤åŒ…æ‹¬ä¸‹è½½æœ€æ–°é©±åŠ¨ã€å¸è½½æ—§é©±åŠ¨ã€å®‰è£…æ–°é©±åŠ¨å¹¶é‡å¯ç³»ç»Ÿã€‚",
            "MoFAæ˜¯ä¸€ä¸ªåŸºäºDORA-RSçš„æ¨¡å—åŒ–AIä»£ç†æ¡†æ¶ï¼Œæ”¯æŒé€šè¿‡dataflowé…ç½®è¿æ¥ä¸åŒçš„èŠ‚ç‚¹å’Œä»£ç†ã€‚",
            "Pythonè™šæ‹Ÿç¯å¢ƒç®¡ç†å¯ä»¥ä½¿ç”¨condaã€venvæˆ–poetryç­‰å·¥å…·ï¼Œæ¨èä½¿ç”¨condaè¿›è¡Œç¯å¢ƒéš”ç¦»ã€‚"
        ]
        
        print("ğŸ“ æ’å…¥æµ‹è¯•æ–‡æ¡£...")
        for i, doc in enumerate(test_docs, 1):
            try:
                await rag.ainsert(doc)
                print(f"âœ… æ’å…¥æ–‡æ¡£ {i}: {doc[:50]}...")
            except Exception as e:
                print(f"âŒ æ’å…¥æ–‡æ¡£ {i} å¤±è´¥: {e}")
        
        # äº¤äº’å¼æŸ¥è¯¢
        print("\nğŸ‰ ç³»ç»Ÿå°±ç»ªï¼å¼€å§‹äº¤äº’å¼é—®ç­”")
        print("ğŸ’¡ è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            try:
                question = input("\nğŸ—£ï¸  è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if not question:
                    continue
                    
                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                print(f"ğŸ” æŸ¥è¯¢ä¸­...")
                
                # æ‰§è¡ŒæŸ¥è¯¢
                try:
                    response = await rag.aquery(question)
                    print("=" * 60)
                    print("ğŸ¯ æŸ¥è¯¢ç»“æœ")
                    print("=" * 60)
                    print(f"â“ é—®é¢˜: {question}")
                    print(f"ğŸ¤– å›ç­”: {response}")
                    print("=" * 60)
                except Exception as e:
                    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†å‡ºé”™: {e}")
    
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        logging.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())