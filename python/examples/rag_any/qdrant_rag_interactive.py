#!/usr/bin/env python3
"""
äº¤äº’å¼ Qdrant RAG é—®ç­”ç³»ç»Ÿ
ç´¢å¼•çŸ¥è¯†åº“åæä¾›æŒç»­çš„é—®ç­”æœåŠ¡
"""

import os
import asyncio
import json
import re
from pathlib import Path
from typing import List, Dict, Any
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import httpx
from datetime import datetime

# é¿å…ä»£ç†é—®é¢˜
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

class InteractiveRAG:
    """äº¤äº’å¼ RAG ç³»ç»Ÿ"""
    
    def __init__(
        self,
        knowledge_base_path: str,
        collection_name: str = "knowledge_base",
        ollama_host: str = "10.100.1.115:11434",
        ollama_model: str = "gpt-oss:20b"
    ):
        self.knowledge_base_path = Path(knowledge_base_path)
        self.collection_name = collection_name
        self.ollama_host = ollama_host
        self.ollama_model = ollama_model
        self.ollama_url = f"http://{ollama_host}/api/generate"
        
        # åˆå§‹åŒ– Qdrant
        self.client = QdrantClient(
            host="localhost", 
            port=6333,
            timeout=60,
            prefer_grpc=False
        )
        
        # è¯æ±‡è¡¨å’Œå‘é‡åŒ–
        self.vocabulary = {}
        self.vocab_size = 1000
        
        # å·¥ä½œç›®å½•
        self.working_dir = Path("./rag_storage")
        self.working_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ äº¤äº’å¼ RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def setup_collection(self):
        """è®¾ç½® Qdrant é›†åˆ"""
        try:
            collections = self.client.get_collections()
            collection_exists = any(c.name == self.collection_name for c in collections.collections)
            
            if collection_exists:
                info = self.client.get_collection(self.collection_name)
                print(f"âœ… ä½¿ç”¨ç°æœ‰é›†åˆ '{self.collection_name}' ({info.points_count} ä¸ªæ–‡æ¡£)")
                return True
            
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=self.vocab_size, distance=Distance.COSINE)
            )
            print(f"âœ… åˆ›å»ºæ–°é›†åˆ '{self.collection_name}'")
            return True
            
        except Exception as e:
            print(f"âŒ é›†åˆè®¾ç½®å¤±è´¥: {e}")
            return False
    
    def get_supported_files(self) -> List[Path]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶"""
        files = []
        for file_path in self.knowledge_base_path.rglob("*.md"):
            if (file_path.is_file() and 
                not file_path.name.startswith('.') and
                file_path.stat().st_size > 50):
                files.append(file_path)
        return sorted(files)[:100]  # é™åˆ¶æ•°é‡
    
    def build_vocabulary(self, texts: List[str]) -> Dict[str, int]:
        """æ„å»ºè¯æ±‡è¡¨"""
        word_freq = {}
        for text in texts:
            words = re.findall(r'\w+', text.lower())
            for word in words:
                if len(word) > 2:
                    word_freq[word] = word_freq.get(word, 0) + 1
        
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        vocabulary = {word: i for i, (word, _) in enumerate(sorted_words[:self.vocab_size])}
        return vocabulary
    
    def text_to_vector(self, text: str) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–"""
        vector = [0.0] * len(self.vocabulary)
        words = re.findall(r'\w+', text.lower())
        
        for word in words:
            if word in self.vocabulary:
                vector[self.vocabulary[word]] += 1.0
        
        total = sum(vector)
        if total > 0:
            vector = [v / total for v in vector]
        
        while len(vector) < self.vocab_size:
            vector.append(0.0)
        
        return vector[:self.vocab_size]
    
    def chunk_text(self, text: str, chunk_size: int = 300) -> List[str]:
        """æ–‡æœ¬åˆ†å—"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(current_chunk) + len(para) > chunk_size and current_chunk:
                chunks.append(current_chunk)
                current_chunk = para
            else:
                current_chunk += "\n" + para if current_chunk else para
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 20]
    
    async def index_documents(self):
        """ç´¢å¼•æ–‡æ¡£"""
        print("ğŸ“š å¼€å§‹ç´¢å¼•æ–‡æ¡£...")
        
        # è·å–æ–‡ä»¶
        files = self.get_supported_files()
        print(f"ğŸ“ å‘ç° {len(files)} ä¸ªæ–‡ä»¶")
        
        # å¤„ç†æ–‡æ¡£
        all_texts = []
        file_chunks = []
        
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                chunks = self.chunk_text(content)
                for i, chunk in enumerate(chunks):
                    all_texts.append(chunk)
                    file_chunks.append({
                        'file_path': str(file_path.relative_to(self.knowledge_base_path)),
                        'file_name': file_path.name,
                        'chunk_index': i,
                        'content': chunk
                    })
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path.name}: {e}")
        
        if not all_texts:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
            return False
        
        print(f"âœ… å¤„ç†äº† {len(all_texts)} ä¸ªæ–‡æ¡£å—")
        
        # æ„å»ºè¯æ±‡è¡¨
        print("ğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
        self.vocabulary = self.build_vocabulary(all_texts)
        print(f"âœ… è¯æ±‡è¡¨å®Œæˆ ({len(self.vocabulary)} ä¸ªè¯)")
        
        # å‘é‡åŒ–å¹¶æ’å…¥
        print("ğŸ”¢ ç”Ÿæˆå‘é‡å¹¶æ’å…¥æ•°æ®åº“...")
        points = []
        
        for i, (text, meta) in enumerate(zip(all_texts, file_chunks)):
            vector = self.text_to_vector(text)
            points.append(PointStruct(
                id=i,
                vector=vector,
                payload={**meta, 'indexed_at': datetime.now().isoformat()}
            ))
        
        # æ‰¹é‡æ’å…¥
        batch_size = 20
        total_inserted = 0
        
        for i in range(0, len(points), batch_size):
            batch = points[i:i+batch_size]
            self.client.upsert(collection_name=self.collection_name, points=batch)
            total_inserted += len(batch)
            print(f"ğŸ“¥ å·²æ’å…¥ {total_inserted}/{len(points)} ä¸ªæ–‡æ¡£å—")
            await asyncio.sleep(0.1)
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_file = self.working_dir / "vocabulary.json"
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(self.vocabulary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ‰ ç´¢å¼•å®Œæˆï¼å…± {total_inserted} ä¸ªæ–‡æ¡£å—")
        return True
    
    def load_vocabulary(self):
        """åŠ è½½è¯æ±‡è¡¨"""
        vocab_file = self.working_dir / "vocabulary.json"
        if vocab_file.exists():
            with open(vocab_file, 'r', encoding='utf-8') as f:
                self.vocabulary = json.load(f)
            print(f"ğŸ“– åŠ è½½è¯æ±‡è¡¨ ({len(self.vocabulary)} ä¸ªè¯)")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°è¯æ±‡è¡¨ï¼Œéœ€è¦é‡æ–°ç´¢å¼•")
    
    def search_documents(self, query: str, limit: int = 5) -> List[Dict]:
        """æœç´¢æ–‡æ¡£"""
        try:
            query_vector = self.text_to_vector(query)
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=limit,
                with_payload=True
            )
            
            return [{
                "score": result.score,
                "content": result.payload["content"],
                "file_path": result.payload["file_path"],
                "file_name": result.payload["file_name"],
                "chunk_index": result.payload.get("chunk_index", 0)
            } for result in results]
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    async def query_ollama(self, prompt: str) -> str:
        """æŸ¥è¯¢ Ollama"""
        payload = {
            "model": self.ollama_model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": 800,
                "temperature": 0.7
            }
        }
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(self.ollama_url, json=payload)
                if response.status_code == 200:
                    return response.json().get("response", "æœªè·å–åˆ°å“åº”")
                else:
                    return f"âŒ Ollama é”™è¯¯ ({response.status_code})"
        except Exception as e:
            return f"âŒ è¿æ¥ Ollama å¤±è´¥: {e}"
    
    async def answer_question(self, question: str) -> Dict[str, Any]:
        """å›ç­”é—®é¢˜"""
        print(f"\nğŸ¤” é—®é¢˜: {question}")
        print("ğŸ” æœç´¢ç›¸å…³æ–‡æ¡£...")
        
        # æœç´¢æ–‡æ¡£
        search_results = self.search_documents(question, limit=5)
        
        if not search_results:
            return {
                "question": question,
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "rag_context": "æ— ç›¸å…³æ–‡æ¡£"
            }
        
        # è¿‡æ»¤ç»“æœ
        filtered_results = [r for r in search_results if r['score'] > 0.05]
        if not filtered_results:
            filtered_results = search_results[:3]
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(filtered_results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        sources = []
        
        for i, result in enumerate(filtered_results, 1):
            context_parts.append(f"ã€å‚è€ƒæ–‡æ¡£{i}ã€‘")
            context_parts.append(f"æ–‡ä»¶: {result['file_name']}")
            context_parts.append(f"å†…å®¹: {result['content']}")
            context_parts.append("---")
            
            sources.append({
                "file": result['file_path'],
                "file_name": result['file_name'],
                "score": result['score'],
                "preview": result['content'][:80] + "..."
            })
        
        context = '\n'.join(context_parts)
        
        # RAG æç¤º
        rag_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„çŸ¥è¯†åº“æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å›ç­”è¦æ±‚:
1. ä»…åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å†…å®¹å›ç­”
2. å›ç­”è¦å‡†ç¡®ã€è¯¦ç»†ã€æ˜“æ‡‚
3. å¯ä»¥å¼•ç”¨å…·ä½“æ–‡æ¡£å†…å®¹
4. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜
5. ç”¨ä¸­æ–‡å›ç­”

å‚è€ƒæ–‡æ¡£:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·å›ç­”:"""
        
        print("ğŸ¤– AI æ­£åœ¨æ€è€ƒ...")
        answer = await self.query_ollama(rag_prompt)
        
        return {
            "question": question,
            "answer": answer,
            "sources": sources,
            "rag_context": context,
            "filtered_results_count": len(filtered_results)
        }
    
    def print_rag_result(self, result: Dict[str, Any]):
        """æ‰“å° RAG ç»“æœ"""
        print("=" * 80)
        print("ğŸ¯ RAG é—®ç­”ç»“æœ")
        print("=" * 80)
        
        print(f"â“ é—®é¢˜: {result['question']}")
        print(f"\nğŸ¤– å›ç­”:\n{result['answer']}")
        
        if result['sources']:
            print(f"\nğŸ“š å‚è€ƒæ–‡æ¡£ ({result['filtered_results_count']} ä¸ªç»“æœ):")
            for i, source in enumerate(result['sources'], 1):
                print(f"   {i}. {source['file_name']} (ç›¸ä¼¼åº¦: {source['score']:.3f})")
                print(f"      é¢„è§ˆ: {source['preview']}")
        
        print(f"\nğŸ“ RAG ä¸Šä¸‹æ–‡ (å‰200å­—ç¬¦):")
        context_preview = result['rag_context'][:200] + "..." if len(result['rag_context']) > 200 else result['rag_context']
        print(f"   {context_preview}")
        
        print("=" * 80)
    
    async def interactive_session(self):
        """äº¤äº’å¼é—®ç­”ä¼šè¯"""
        print("\nğŸ‰ æ¬¢è¿ä½¿ç”¨äº¤äº’å¼ RAG é—®ç­”ç³»ç»Ÿï¼")
        print("ğŸ’¡ æç¤º:")
        print("   - ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢")
        print("   - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("   - è¾“å…¥ 'stats' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("   - è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        
        while True:
            try:
                user_input = input("\nğŸ—£ï¸  è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨ RAG é—®ç­”ç³»ç»Ÿï¼")
                    break
                
                if user_input.lower() == 'stats':
                    try:
                        info = self.client.get_collection(self.collection_name)
                        print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€:")
                        print(f"   - æ–‡æ¡£å—æ•°é‡: {info.points_count}")
                        print(f"   - è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")
                        print(f"   - é›†åˆåç§°: {self.collection_name}")
                    except Exception as e:
                        print(f"âŒ è·å–çŠ¶æ€å¤±è´¥: {e}")
                    continue
                
                if user_input.lower() == 'help':
                    print("ğŸ“– å¸®åŠ©ä¿¡æ¯:")
                    print("   - è¿™æ˜¯åŸºäºä½ çš„çŸ¥è¯†åº“çš„ RAG é—®ç­”ç³»ç»Ÿ")
                    print("   - ç³»ç»Ÿä¼šæœç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åç”¨ AI ç”Ÿæˆå›ç­”")
                    print("   - å°è¯•é—®æŠ€æœ¯ç›¸å…³é—®é¢˜ï¼Œå¦‚ 'Pythonç¼–ç¨‹'ã€'Dockerä½¿ç”¨' ç­‰")
                    print("   - ç³»ç»Ÿä¼šæ˜¾ç¤ºå®Œæ•´çš„ RAG å¤„ç†è¿‡ç¨‹")
                    continue
                
                # å¤„ç†é—®é¢˜
                result = await self.answer_question(user_input)
                self.print_rag_result(result)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†å‡ºé”™: {e}")

async def main():
    print("ğŸš€ äº¤äº’å¼ Qdrant RAG çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    print("=" * 50)
    
    # é…ç½®
    KNOWLEDGE_BASE_PATH = "/Users/chenzi/project/chenzi-knowledge/chenzi-knowledge-library"
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = InteractiveRAG(KNOWLEDGE_BASE_PATH)
    
    # è®¾ç½®é›†åˆ
    if not rag.setup_collection():
        print("âŒ æ— æ³•è®¾ç½® Qdrant é›†åˆ")
        return
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç´¢å¼•
    try:
        info = rag.client.get_collection(rag.collection_name)
        if info.points_count == 0:
            print("ğŸ“š é›†åˆä¸ºç©ºï¼Œå¼€å§‹ç´¢å¼•æ–‡æ¡£...")
            success = await rag.index_documents()
            if not success:
                print("âŒ ç´¢å¼•å¤±è´¥")
                return
        else:
            print(f"ğŸ“– ä½¿ç”¨ç°æœ‰ç´¢å¼• ({info.points_count} ä¸ªæ–‡æ¡£)")
            rag.load_vocabulary()
    except:
        print("ğŸ“š å¼€å§‹ç´¢å¼•æ–‡æ¡£...")
        success = await rag.index_documents()
        if not success:
            print("âŒ ç´¢å¼•å¤±è´¥")
            return
    
    # å¼€å§‹äº¤äº’å¼ä¼šè¯
    await rag.interactive_session()

if __name__ == "__main__":
    asyncio.run(main())