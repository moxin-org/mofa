#!/usr/bin/env python3
"""
LightRAG äº¤äº’å¼é—®ç­”ç³»ç»Ÿ
åŸºäº LightRAG æ¡†æ¶æ„å»ºçš„é«˜æ•ˆçŸ¥è¯†æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿ
æ”¯æŒå¤šç§æŸ¥è¯¢æ¨¡å¼ï¼šhybridã€localã€globalã€naive
"""

import os
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import httpx
import numpy as np

# å¯¼å…¥ LightRAG æ ¸å¿ƒç»„ä»¶
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

# å¯¼å…¥ Qdrant å®¢æˆ·ç«¯
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# é¿å…ä»£ç†é—®é¢˜
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

# é…ç½® - ä½¿ç”¨ä½ çš„ Ollama æœåŠ¡å™¨å’Œ Qdrant æ•°æ®åº“
OLLAMA_LLM_HOST = "10.100.1.115:11434"
OLLAMA_EMBED_HOST = "10.100.1.115:11435"
LLM_MODEL = "gpt-oss:20b"
EMBED_MODEL = "dengcao/Qwen3-Embedding-8B:Q5_K_M"

# Qdrant é…ç½®
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
QDRANT_COLLECTION_NAME = "lightrag_8192"  # 8192ç»´å‘é‡ä¸“ç”¨é›†åˆ


async def ollama_llm_func(prompt, system_prompt=None, history_messages=None, **kwargs):
    """LightRAG å…¼å®¹çš„ Ollama LLM å‡½æ•°"""
    try:
        # æ„å»ºå®Œæ•´çš„æç¤º
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"

        # æ·»åŠ å†å²æ¶ˆæ¯
        if history_messages:
            history_text = "\n".join(
                [f"{msg.get('role', 'user')}: {msg.get('content', '')}" for msg in history_messages])
            full_prompt = f"{history_text}\n\nUser: {prompt}"

        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"http://{OLLAMA_LLM_HOST}/api/generate",
                json={
                    "model": LLM_MODEL,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 1000,
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                }
            )
            if response.status_code == 200:
                return response.json().get("response", "No response")
            else:
                return f"Error: {response.status_code}"
    except Exception as e:
        logging.error(f"LLM è¯·æ±‚å¤±è´¥: {e}")
        return f"LLM Error: {str(e)}"


async def ollama_embed_func(texts, **kwargs):
    """ä¿®å¤çš„ Ollama åµŒå…¥å‡½æ•°"""
    if isinstance(texts, str):
        texts = [texts]

    results = []

    # ä½¿ç”¨æ­£ç¡®çš„ API ç«¯ç‚¹
    async with httpx.AsyncClient(timeout=60.0) as client:
        for text in texts:
            try:
                response = await client.post(
                    f"http://{OLLAMA_EMBED_HOST}/api/embed",  # â† ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç«¯ç‚¹
                    json={
                        "model": EMBED_MODEL,
                        "input": text  # â† ä¿®å¤ï¼šä½¿ç”¨ input è€Œä¸æ˜¯ prompt
                    }
                )
                if response.status_code == 200:
                    embedding = response.json().get("embeddings", [])
                    if embedding and len(embedding) > 0:
                        results.append(embedding[0])
                    else:
                        logging.error(f"âŒ ç©ºçš„ embedding å“åº”")
                        return None  # ä¸ä½¿ç”¨éšæœºå‘é‡
                else:
                    logging.error(f"âŒ Embedding API é”™è¯¯: {response.status_code}")
                    return None

            except Exception as e:
                logging.error(f"âŒ åµŒå…¥è¯·æ±‚å¤±è´¥: {e}")
                return None

    return results


class LightRAGSystem:
    """LightRAG ç³»ç»Ÿä¸»ç±» - é›†æˆ Qdrant å‘é‡æ•°æ®åº“"""

    def __init__(self, working_dir: str = "./lightrag_storage", knowledge_base_path: str = None):
        self.working_dir = Path(working_dir)
        self.knowledge_base_path = Path(knowledge_base_path) if knowledge_base_path else None
        self.rag = None
        self.qdrant_client = None

        # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
        self.working_dir.mkdir(exist_ok=True)

        logging.info(f"ğŸš€ LightRAG ç³»ç»Ÿåˆå§‹åŒ–")
        logging.info(f"   - å·¥ä½œç›®å½•: {self.working_dir}")
        logging.info(f"   - çŸ¥è¯†åº“è·¯å¾„: {self.knowledge_base_path}")
        logging.info(f"   - LLM: {LLM_MODEL} @ {OLLAMA_LLM_HOST}")
        logging.info(f"   - åµŒå…¥: {EMBED_MODEL} @ {OLLAMA_EMBED_HOST}")
        logging.info(f"   - Qdrant: {QDRANT_HOST}:{QDRANT_PORT}/{QDRANT_COLLECTION_NAME}")
    
    def setup_qdrant(self) -> bool:
        """è®¾ç½® Qdrant å‘é‡æ•°æ®åº“"""
        try:
            # åˆ›å»º Qdrant å®¢æˆ·ç«¯
            self.qdrant_client = QdrantClient(
                host=QDRANT_HOST,
                port=QDRANT_PORT,
                timeout=60
            )
            
            # é¦–å…ˆæµ‹è¯•è¿æ¥
            logging.info("ğŸ”Œ æµ‹è¯• Qdrant è¿æ¥...")
            collections = self.qdrant_client.get_collections()
            logging.info(f"ğŸ“¦ å½“å‰ Qdrant ä¸­æœ‰ {len(collections.collections)} ä¸ªé›†åˆ")
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collection_exists = any(c.name == QDRANT_COLLECTION_NAME for c in collections.collections)
            
            if collection_exists:
                info = self.qdrant_client.get_collection(QDRANT_COLLECTION_NAME)
                logging.info(f"âœ… ä½¿ç”¨ç°æœ‰ Qdrant é›†åˆ: {QDRANT_COLLECTION_NAME} ({info.points_count} ä¸ªå‘é‡)")
            else:
                # åˆ›å»ºæ–°é›†åˆ
                logging.info(f"ğŸ“ åˆ›å»ºæ–°é›†åˆ: {QDRANT_COLLECTION_NAME}")
                try:
                    self.qdrant_client.create_collection(
                        collection_name=QDRANT_COLLECTION_NAME,
                        vectors_config=VectorParams(
                            size=8192,  # Qwen3-Embedding-8B çš„å‘é‡ç»´åº¦
                            distance=Distance.COSINE
                        )
                    )
                    logging.info(f"âœ… åˆ›å»ºæ–° Qdrant é›†åˆ: {QDRANT_COLLECTION_NAME}")
                except Exception as create_error:
                    logging.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {create_error}")
                    # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯è€Œä¸ä¿®æ”¹å…¨å±€å˜é‡
                    return False
            
            return True
            
        except Exception as e:
            logging.error(f"âŒ Qdrant è®¾ç½®å¤±è´¥: {e}")
            return False

    async def initialize(self):
        """åˆå§‹åŒ– LightRAG ç³»ç»Ÿ"""
        try:
            # å°è¯•è®¾ç½® Qdrant å‘é‡æ•°æ®åº“
            qdrant_ok = self.setup_qdrant()
            if not qdrant_ok:
                logging.warning("âš ï¸ Qdrant è®¾ç½®å¤±è´¥ï¼Œç³»ç»Ÿå°†ä»…ä½¿ç”¨ LightRAG å†…ç½®å­˜å‚¨")
                self.qdrant_client = None
            
            # åˆ›å»ºåµŒå…¥å‡½æ•°
            embedding_func = EmbeddingFunc(
                embedding_dim=8192,  # Qwen3-Embedding-8B çš„æ­£ç¡®ç»´åº¦
                max_token_size=8192,
                func=ollama_embed_func
            )

            # åˆ›å»º LightRAG å®ä¾‹
            # æ³¨æ„ï¼šLightRAG å¯èƒ½ä¸ç›´æ¥æ”¯æŒå¤–éƒ¨ Qdrantï¼Œæˆ‘ä»¬å…ˆç”¨é»˜è®¤å­˜å‚¨
            self.rag = LightRAG(
                working_dir=str(self.working_dir),
                llm_model_func=ollama_llm_func,
                embedding_func=embedding_func,
                enable_llm_cache=True
            )
            
            # åˆå§‹åŒ–å­˜å‚¨
            await self.rag.initialize_storages()
            await initialize_pipeline_status()

            logging.info("âœ… LightRAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True

        except Exception as e:
            logging.error(f"âŒ LightRAG åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def check_services(self):
        """æ£€æŸ¥æœåŠ¡è¿é€šæ€§"""
        print("ğŸ” æ£€æŸ¥æœåŠ¡çŠ¶æ€...")

        try:
            import requests

            # æ£€æŸ¥ LLM æœåŠ¡
            llm_response = requests.get(f"http://{OLLAMA_LLM_HOST}/api/tags", timeout=5)
            if llm_response.status_code == 200:
                print("âœ… LLM æœåŠ¡æ­£å¸¸")
            else:
                print(f"âŒ LLM æœåŠ¡å¼‚å¸¸: {llm_response.status_code}")
                return False

            # æ£€æŸ¥ Embedding æœåŠ¡
            embed_response = requests.get(f"http://{OLLAMA_EMBED_HOST}/api/tags", timeout=5)
            if embed_response.status_code == 200:
                print("âœ… Embedding æœåŠ¡æ­£å¸¸")
            else:
                print(f"âŒ Embedding æœåŠ¡å¼‚å¸¸: {embed_response.status_code}")
                return False
            
            # æ£€æŸ¥ Qdrant æœåŠ¡
            qdrant_response = requests.get(f"http://{QDRANT_HOST}:{QDRANT_PORT}/collections", timeout=5)
            if qdrant_response.status_code == 200:
                print("âœ… Qdrant å‘é‡æ•°æ®åº“æ­£å¸¸")
            else:
                print(f"âŒ Qdrant æœåŠ¡å¼‚å¸¸: {qdrant_response.status_code}")
                return False

            return True

        except Exception as e:
            print(f"âŒ æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def insert_document(self, file_path: Path) -> bool:
        """æ’å…¥å•ä¸ªæ–‡æ¡£ - åŒæ—¶å­˜å‚¨åˆ° LightRAG å’Œ Qdrant"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            if content.strip():
                # ä½¿ç”¨ LightRAG å¤„ç†æ–‡æ¡£ï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±æ„å»ºï¼‰
                await self.rag.ainsert(content)
                
                # é¢å¤–å­˜å‚¨åˆ° Qdrantï¼ˆç”¨äºå¿«é€Ÿå‘é‡æ£€ç´¢ï¼‰
                await self._store_to_qdrant(content, str(file_path))
                
                logging.info(f"âœ… å·²æ’å…¥æ–‡æ¡£: {file_path.name} (LightRAG + Qdrant)")
                return True
            else:
                logging.warning(f"âš ï¸ æ–‡æ¡£ä¸ºç©ºï¼Œè·³è¿‡: {file_path.name}")
                return False

        except Exception as e:
            logging.error(f"âŒ æ’å…¥æ–‡æ¡£å¤±è´¥ {file_path.name}: {e}")
            return False
    
    async def _store_to_qdrant(self, content: str, file_path: str):
        """å°†æ–‡æ¡£å‘é‡å­˜å‚¨åˆ° Qdrant"""
        try:
            if not self.qdrant_client:
                return
            
            # ç”Ÿæˆå†…å®¹çš„åµŒå…¥å‘é‡
            embeddings = await ollama_embed_func([content])
            if not embeddings or not embeddings[0]:
                logging.warning(f"âš ï¸ æ— æ³•ç”ŸæˆåµŒå…¥å‘é‡: {file_path}")
                return
            
            # åˆ›å»ºç‚¹IDï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œï¼‰
            import hashlib
            point_id = int(hashlib.md5(file_path.encode()).hexdigest()[:8], 16)
            
            # æ’å…¥åˆ° Qdrant
            point = PointStruct(
                id=point_id,
                vector=embeddings[0],
                payload={
                    "content": content[:1000],  # å­˜å‚¨å‰1000å­—ç¬¦ä½œä¸ºé¢„è§ˆ
                    "file_path": file_path,
                    "timestamp": str(datetime.now())
                }
            )
            
            self.qdrant_client.upsert(
                collection_name=QDRANT_COLLECTION_NAME,
                points=[point]
            )
            
            logging.debug(f"âœ… å‘é‡å·²å­˜å‚¨åˆ° Qdrant: {file_path}")
            
        except Exception as e:
            logging.warning(f"âš ï¸ Qdrant å­˜å‚¨å¤±è´¥: {e}")
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ

    async def process_knowledge_base(self) -> Dict[str, Any]:
        """å¤„ç†çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        if not self.knowledge_base_path or not self.knowledge_base_path.exists():
            return {'success': False, 'error': 'çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨'}

        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ï¼ˆä¸“æ³¨äºç®€å•æ–‡æœ¬æ–‡ä»¶ï¼Œé¿å…å¤æ‚è§£æï¼‰
            supported_extensions = ['.md', '.txt', '.rtf']
            files = []

            for ext in supported_extensions:
                files.extend(list(self.knowledge_base_path.rglob(f"*{ext}")))

            if not files:
                return {'success': False, 'error': 'æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶'}

            logging.info(f"ğŸ“š å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡æ¡£...")

            success_count = 0
            for i, file_path in enumerate(files, 1):
                print(f"ğŸ“„ å¤„ç†æ–‡æ¡£ {i}/{len(files)}: {file_path.name}")
                if await self.insert_document(file_path):
                    success_count += 1

                # æ¯å¤„ç†5ä¸ªæ–‡æ¡£æ˜¾ç¤ºè¿›åº¦
                if i % 5 == 0:
                    print(f"ğŸ“Š è¿›åº¦: {i}/{len(files)} ({success_count} ä¸ªæˆåŠŸ)")

            logging.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {success_count}/{len(files)} ä¸ªæˆåŠŸ")

            return {
                'success': True,
                'total_files': len(files),
                'success_count': success_count,
                'failed_count': len(files) - success_count
            }

        except Exception as e:
            logging.error(f"âŒ å¤„ç†çŸ¥è¯†åº“å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    async def query(self, question: str, mode: str = "hybrid") -> Dict[str, Any]:
        """å¢å¼ºæŸ¥è¯¢ - ç»“åˆ LightRAG å’Œ Qdrant"""
        try:
            if not self.rag:
                return {
                    'question': question,
                    'answer': 'âŒ LightRAG ç³»ç»Ÿæœªåˆå§‹åŒ–',
                    'mode': mode,
                    'status': 'error'
                }

            logging.info(f"ğŸ” æŸ¥è¯¢: {question} (æ¨¡å¼: {mode})")

            # ä½¿ç”¨ LightRAG æŸ¥è¯¢ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
            try:
                # å°è¯•ä½¿ç”¨ mode å‚æ•°
                response = await self.rag.aquery(question, param=QueryParam())
            except TypeError:
                # å¦‚æœä¸æ”¯æŒ QueryParamï¼Œä½¿ç”¨ç®€å•æŸ¥è¯¢
                logging.warning(f"âš ï¸ ä¸æ”¯æŒæŸ¥è¯¢æ¨¡å¼ {mode}ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢")
                response = await self.rag.aquery(question)
            
            # å¯é€‰ï¼šä½¿ç”¨ Qdrant è¿›è¡Œé¢å¤–çš„å‘é‡æ£€ç´¢éªŒè¯
            qdrant_results = await self._query_qdrant(question, limit=3)

            return {
                'question': question,
                'answer': response,
                'mode': mode,
                'qdrant_results_count': len(qdrant_results) if qdrant_results else 0,
                'status': 'success'
            }

        except Exception as e:
            logging.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'question': question,
                'answer': f'æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}',
                'mode': mode,
                'status': 'error'
            }
    
    async def _query_qdrant(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ Qdrant è¿›è¡Œå‘é‡æ£€ç´¢"""
        try:
            if not self.qdrant_client:
                return []
            
            # ç”Ÿæˆé—®é¢˜çš„åµŒå…¥å‘é‡
            embeddings = await ollama_embed_func([question])
            if not embeddings or not embeddings[0]:
                return []
            
            # åœ¨ Qdrant ä¸­æœç´¢ç›¸ä¼¼å‘é‡
            search_result = self.qdrant_client.search(
                collection_name=QDRANT_COLLECTION_NAME,
                query_vector=embeddings[0],
                limit=limit,
                with_payload=True,
                score_threshold=0.3  # ç›¸ä¼¼åº¦é˜ˆå€¼
            )
            
            results = []
            for result in search_result:
                results.append({
                    'score': result.score,
                    'content': result.payload['content'],
                    'file_path': result.payload['file_path'],
                    'timestamp': result.payload.get('timestamp', '')
                })
            
            logging.debug(f"ğŸ” Qdrant æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼ç»“æœ")
            return results
            
        except Exception as e:
            logging.warning(f"âš ï¸ Qdrant æŸ¥è¯¢å¤±è´¥: {e}")
            return []

    def print_query_result(self, result: Dict[str, Any]):
        """æ‰“å°æŸ¥è¯¢ç»“æœ"""
        print("=" * 80)
        print("ğŸ¯ LightRAG + Qdrant æŸ¥è¯¢ç»“æœ")
        print("=" * 80)

        print(f"â“ é—®é¢˜: {result['question']}")
        print(f"ğŸ” æŸ¥è¯¢æ¨¡å¼: {result['mode']}")
        print(f"ğŸ“Š çŠ¶æ€: {result['status']}")
        
        if result.get('qdrant_results_count', 0) > 0:
            print(f"ğŸ—„ï¸ Qdrant è¾…åŠ©æ£€ç´¢: {result['qdrant_results_count']} ä¸ªç›¸å…³ç»“æœ")
        
        print(f"\nğŸ¤– å›ç­”:\n{result['answer']}")
        print("=" * 80)

    async def interactive_session(self):
        """äº¤äº’å¼ä¼šè¯"""
        print("\nğŸ‰ æ¬¢è¿ä½¿ç”¨ LightRAG äº¤äº’å¼é—®ç­”ç³»ç»Ÿï¼")
        print("ğŸ’¡ æç¤º:")
        print("   - ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢")
        print("   - è¾“å…¥ 'mode:æ¨¡å¼ é—®é¢˜' æŒ‡å®šæŸ¥è¯¢æ¨¡å¼")
        print("   - æ”¯æŒæ¨¡å¼: hybrid (æ··åˆ), local (å±€éƒ¨), global (å…¨å±€), naive (æœ´ç´ )")
        print("   - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("   - è¾“å…¥ 'stats' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("   - è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        print("   - è¾“å…¥ 'reindex' é‡æ–°ç´¢å¼•æ–‡æ¡£")

        while True:
            try:
                user_input = input("\nğŸ—£ï¸  è¯·è¾“å…¥é—®é¢˜: ").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨ LightRAG é—®ç­”ç³»ç»Ÿï¼")
                    break

                if user_input.lower() == 'stats':
                    print("ğŸ“Š LightRAG + Qdrant ç³»ç»ŸçŠ¶æ€:")
                    print(f"   - å·¥ä½œç›®å½•: {self.working_dir}")
                    print(f"   - çŸ¥è¯†åº“è·¯å¾„: {self.knowledge_base_path}")
                    print(f"   - LLM æ¨¡å‹: {LLM_MODEL}")
                    print(f"   - åµŒå…¥æ¨¡å‹: {EMBED_MODEL}")
                    print(f"   - Qdrant æ•°æ®åº“: {QDRANT_HOST}:{QDRANT_PORT}/{QDRANT_COLLECTION_NAME}")
                    print(f"   - æ”¯æŒæŸ¥è¯¢æ¨¡å¼: hybrid, local, global, naive")
                    
                    # æ˜¾ç¤º Qdrant é›†åˆä¿¡æ¯
                    if self.qdrant_client:
                        try:
                            info = self.qdrant_client.get_collection(QDRANT_COLLECTION_NAME)
                            print(f"   - Qdrant å‘é‡æ•°: {info.points_count}")
                        except:
                            print(f"   - Qdrant çŠ¶æ€: è¿æ¥å¼‚å¸¸")
                    continue

                if user_input.lower() == 'help':
                    print("ğŸ“– LightRAG ç³»ç»Ÿå¸®åŠ©:")
                    print("   - ğŸ¯ åŸºäºå›¾å¢å¼ºçš„é«˜æ•ˆ RAG ç³»ç»Ÿ")
                    print("   - ğŸ“„ æ”¯æŒæ–‡æ¡£: MD, TXT, RTF")
                    print("   - ğŸ” æŸ¥è¯¢æ¨¡å¼:")
                    print("     â€¢ hybrid: æ··åˆæ¨¡å¼ï¼Œç»“åˆå‘é‡å’Œå›¾æ£€ç´¢ (æ¨è)")
                    print("     â€¢ local: å±€éƒ¨æœç´¢ï¼ŒåŸºäºå®ä½“å…³ç³»")
                    print("     â€¢ global: å…¨å±€æœç´¢ï¼ŒåŸºäºç¤¾åŒºæ€»ç»“")
                    print("     â€¢ naive: æœ´ç´ æœç´¢ï¼Œçº¯å‘é‡æ£€ç´¢")
                    print("   - ğŸ’¡ ç¤ºä¾‹æŸ¥è¯¢:")
                    print("     â€¢ ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?")
                    print("     â€¢ mode:local æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µ")
                    print("     â€¢ mode:global æ€»ç»“æ‰€æœ‰æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹")
                    continue

                if user_input.lower() == 'reindex':
                    print("ğŸ”„ å¼€å§‹é‡æ–°ç´¢å¼•æ–‡æ¡£...")
                    result = await self.process_knowledge_base()
                    if result['success']:
                        print(f"âœ… é‡æ–°ç´¢å¼•å®Œæˆ: {result['success_count']}/{result['total_files']} ä¸ªæ–‡æ¡£")
                    else:
                        print(f"âŒ é‡æ–°ç´¢å¼•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    continue

                # è§£ææŸ¥è¯¢æ¨¡å¼
                mode = "hybrid"  # é»˜è®¤æ¨¡å¼
                question = user_input

                if user_input.startswith("mode:"):
                    parts = user_input.split(" ", 1)
                    if len(parts) == 2:
                        mode_part = parts[0].replace("mode:", "")
                        if mode_part in ["hybrid", "local", "global", "naive"]:
                            mode = mode_part
                            question = parts[1]
                        else:
                            print(f"âš ï¸ æœªçŸ¥æŸ¥è¯¢æ¨¡å¼: {mode_part}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼ hybrid")
                            question = user_input

                # æ‰§è¡ŒæŸ¥è¯¢
                result = await self.query(question, mode=mode)
                self.print_query_result(result)

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                logging.error(f"âŒ å¤„ç†å‡ºé”™: {e}")
                print(f"âŒ å¤„ç†å‡ºé”™: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('lightrag_system.log', encoding='utf-8')
        ]
    )

    print("ğŸš€ LightRAG é«˜æ•ˆçŸ¥è¯†æ£€ç´¢é—®ç­”ç³»ç»Ÿ")
    print("åŸºäºå›¾å¢å¼ºçš„å…ˆè¿› RAG æŠ€æœ¯")
    print("=" * 60)

    # é…ç½®
    KNOWLEDGE_BASE_PATH = "/Users/chenzi/chenzi/project/github/chenzi-knowledge-library"

    try:
        # åˆå§‹åŒ– LightRAG ç³»ç»Ÿ
        rag_system = LightRAGSystem(
            working_dir="./lightrag_storage",
            knowledge_base_path=KNOWLEDGE_BASE_PATH
        )

        # æ£€æŸ¥æœåŠ¡è¿é€šæ€§
        if not rag_system.check_services():
            print("âŒ è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£å¸¸è¿è¡Œ")
            return

        # åˆå§‹åŒ–ç³»ç»Ÿ
        if not await rag_system.initialize():
            logging.error("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return

        # å¤„ç†çŸ¥è¯†åº“æ–‡æ¡£
        logging.info("ğŸ“š å¼€å§‹å¤„ç†çŸ¥è¯†åº“æ–‡æ¡£...")
        result = await rag_system.process_knowledge_base()
        if result['success']:
            logging.info(f"âœ… çŸ¥è¯†åº“å¤„ç†å®Œæˆ: {result['success_count']}/{result['total_files']} ä¸ªæ–‡æ¡£")
        else:
            logging.warning(f"âš ï¸ çŸ¥è¯†åº“å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        # å¼€å§‹äº¤äº’å¼ä¼šè¯
        await rag_system.interactive_session()

    except Exception as e:
        logging.error(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())
