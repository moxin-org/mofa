#!/usr/bin/env python3
"""
LightRAG äº¤äº’å¼é—®ç­”ç³»ç»Ÿ
åŸºäº LightRAG æ¡†æ¶æ„å»ºçš„é«˜æ•ˆçŸ¥è¯†æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿ
æ”¯æŒå¤šç§æŸ¥è¯¢æ¨¡å¼ï¼šhybridã€localã€globalã€naive
"""

import os
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import httpx

# å¯¼å…¥ç¯å¢ƒå˜é‡åŠ è½½å™¨
from dotenv import load_dotenv

# å¯¼å…¥ LightRAG æ ¸å¿ƒç»„ä»¶
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
OLLAMA_LLM_HOST = os.getenv('OLLAMA_LLM_HOST', '10.100.1.115:11434')
LLM_MODEL = os.getenv('LLM_MODEL', 'gpt-oss:20b')

# OpenAI Embedding é…ç½®
LLM_API_KEY = os.getenv('LLM_API_KEY')
LLM_BASE_URL = os.getenv('LLM_BASE_URL', 'https://api.openai.com/v1')
EMBED_MODEL = os.getenv('EMBED_MODEL', 'text-embedding-3-large')

KNOWLEDGE_BASE_PATH = os.getenv('KNOWLEDGE_BASE_PATH',
                                '/Users/chenzi/project/chenzi-knowledge/chenzi-knowledge-library')
LIGHTRAG_WORKING_DIR = os.getenv('LIGHTRAG_WORKING_DIR', './lightrag_storage')
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
LOG_FILE = os.getenv('LOG_FILE', 'lightrag_system.log')

# Qdrant é…ç½®
USE_QDRANT = os.getenv('USE_QDRANT', 'false').lower() == 'true'
QDRANT_URL = os.getenv('QDRANT_URL', 'http://localhost:6333')
QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')
QDRANT_COLLECTION_NAME = os.getenv('QDRANT_COLLECTION_NAME', 'lightrag_vectors')


async def ollama_llm_func(prompt, system_prompt=None, history_messages=None, **kwargs):
    """LightRAG å…¼å®¹çš„ Ollama LLM å‡½æ•°"""
    try:
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        if history_messages:
            messages.extend(history_messages)

        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=180.0) as client:
            response = await client.post(
                f"http://{OLLAMA_LLM_HOST}/v1",
                json={
                    "model": LLM_MODEL,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "num_predict": 2000,
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "top_k": 40
                    }
                }
            )

            if response.status_code == 200:
                result = response.json()
                message_content = result.get("message", {}).get("content", "")

                if not message_content or message_content.strip() == "":
                    return "Based on the provided context, I can provide information on this topic."

                return message_content
            else:
                logging.error(f"Ollama API é”™è¯¯: {response.status_code}")
                return "I apologize, but I cannot process this request at the moment due to a service error."

    except Exception as e:
        logging.error(f"LLM è¯·æ±‚å¤±è´¥: {e}")
        return "I apologize, but I'm experiencing technical difficulties and cannot provide a response right now."


async def openai_embed_func(texts, **kwargs):
    """ç¬¦åˆ LightRAG æ ‡å‡†çš„ OpenAI åµŒå…¥å‡½æ•°"""
    if isinstance(texts, str):
        texts = [texts]

    results = []
    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json"
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            response = await client.post(
                f"{LLM_BASE_URL}/embeddings",
                headers=headers,
                json={
                    "model": EMBED_MODEL,
                    "input": texts
                }
            )
            if response.status_code == 200:
                data = response.json()
                embeddings = data.get("data", [])
                for item in embeddings:
                    results.append(item["embedding"])

                return results
            else:
                logging.error(f"OpenAI Embedding API é”™è¯¯: {response.status_code}")
                return None

        except Exception as e:
            logging.error(f"OpenAI åµŒå…¥è¯·æ±‚å¤±è´¥: {e}")
            return None


class LightRAGSystem:
    """LightRAG ç³»ç»Ÿä¸»ç±»"""

    def __init__(self, working_dir: str = None, knowledge_base_path: str = None):
        self.working_dir = Path(working_dir or LIGHTRAG_WORKING_DIR)
        self.knowledge_base_path = Path(
            knowledge_base_path or KNOWLEDGE_BASE_PATH) if knowledge_base_path or KNOWLEDGE_BASE_PATH else None
        self.rag = None

        # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
        self.working_dir.mkdir(exist_ok=True)

        logging.info(f"LightRAG ç³»ç»Ÿåˆå§‹åŒ–: {self.working_dir}")

    def check_services(self):
        """æ£€æŸ¥æœåŠ¡è¿é€šæ€§"""
        try:
            import requests

            # æ£€æŸ¥ LLM æœåŠ¡
            llm_response = requests.get(f"http://{OLLAMA_LLM_HOST}/api/tags", timeout=5)
            if llm_response.status_code != 200:
                logging.error(f"LLM æœåŠ¡å¼‚å¸¸: {llm_response.status_code}")
                return False

            # æ£€æŸ¥ OpenAI Embedding æœåŠ¡
            embed_response = requests.get(
                "https://api.openai.com/v1/models",
                headers={"Authorization": f"Bearer {LLM_API_KEY}"},
                timeout=5
            )
            if embed_response.status_code != 200:
                logging.error(f"OpenAI Embedding æœåŠ¡å¼‚å¸¸: {embed_response.status_code}")
                return False

            # æ£€æŸ¥ Qdrant æœåŠ¡
            if USE_QDRANT:
                qdrant_headers = {}
                if QDRANT_API_KEY:
                    qdrant_headers["api-key"] = QDRANT_API_KEY

                qdrant_response = requests.get(
                    f"{QDRANT_URL}/collections",
                    headers=qdrant_headers,
                    timeout=5
                )
                if qdrant_response.status_code != 200:
                    logging.error(f"Qdrant æœåŠ¡å¼‚å¸¸: {qdrant_response.status_code}")
                    return False

            logging.info("æœåŠ¡æ£€æŸ¥é€šè¿‡")
            return True

        except Exception as e:
            logging.error(f"æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def initialize(self):
        """åˆå§‹åŒ– LightRAG ç³»ç»Ÿ"""
        try:
            # åˆ›å»ºåµŒå…¥å‡½æ•°
            embedding_func = EmbeddingFunc(
                embedding_dim=3072,
                max_token_size=8191,
                func=openai_embed_func
            )

            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨ Qdrant
            if USE_QDRANT:
                qdrant_config = {
                    "cosine_better_than_threshold": 0.2
                }

                self.rag = LightRAG(
                    working_dir=str(self.working_dir),
                    llm_model_func=ollama_llm_func,
                    embedding_func=embedding_func,
                    vector_storage="QdrantVectorDBStorage",
                    vector_db_storage_cls_kwargs=qdrant_config,
                    enable_llm_cache=True,
                    max_parallel_insert=os.cpu_count(),
                )
            else:
                self.rag = LightRAG(
                    working_dir=str(self.working_dir),
                    llm_model_func=ollama_llm_func,
                    embedding_func=embedding_func,
                    enable_llm_cache=True
                )

            # åˆå§‹åŒ–å­˜å‚¨å’Œç®¡é“
            await self.rag.initialize_storages()
            await initialize_pipeline_status()

            logging.info("LightRAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True

        except Exception as e:
            logging.error(f"LightRAG åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def process_files_batch(self, files: List[Path], batch_size: int = 10) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
        if not files:
            return {'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†'}

        total_success = 0
        total_failed = 0

        logging.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")

        for i in range(0, len(files), batch_size):
            batch_files = files[i:i + batch_size]

            # æ‰¹é‡è¯»å–æ–‡ä»¶å†…å®¹
            documents = []
            file_paths = []

            for file_path in batch_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if content.strip():
                        documents.append(content)
                        file_paths.append(str(file_path))
                except Exception as e:
                    logging.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
                    total_failed += 1

            # æ‰¹é‡æ’å…¥RAG - ä½¿ç”¨åŒæ­¥æ–¹æ³•è¿›è¡Œæ‰¹é‡æ’å…¥
            if documents:
                try:
                    # LightRAG æ”¯æŒæ‰¹é‡æ’å…¥ - ä½¿ç”¨åŒæ­¥æ–¹æ³•é¿å…äº‹ä»¶å¾ªç¯å†²çª
                    self.rag.insert(documents, file_paths=file_paths)
                    total_success += len(documents)
                    logging.info(f"æ‰¹æ¬¡ {i // batch_size + 1} æ’å…¥æˆåŠŸ: {len(documents)} ä¸ªæ–‡ä»¶")
                except Exception as e:
                    logging.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
                    total_failed += len(documents)

        logging.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {total_success}, å¤±è´¥ {total_failed}")

        return {
            'success': True,
            'total_files': len(files),
            'success_count': total_success,
            'failed_count': total_failed
        }

    async def process_knowledge_base(self) -> Dict[str, Any]:
        """å¤„ç†çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        if not self.knowledge_base_path or not self.knowledge_base_path.exists():
            return {'success': False, 'error': f'çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {self.knowledge_base_path}'}

        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
            supported_extensions = ['.md', '.txt', '.rtf', '.py', '.json']
            files = []

            for ext in supported_extensions:
                files.extend(list(self.knowledge_base_path.rglob(f"*{ext}")))

            if not files:
                return {'success': False, 'error': 'æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶'}

            # ä½¿ç”¨æ‰¹é‡å¤„ç†
            return await self.process_files_batch(files, batch_size=15)

        except Exception as e:
            logging.error(f"å¤„ç†çŸ¥è¯†åº“å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    async def query(self, question: str, mode: str = "hybrid") -> Dict[str, Any]:
        """LightRAG æŸ¥è¯¢"""
        try:
            if not self.rag:
                return {
                    'question': question,
                    'answer': 'LightRAG ç³»ç»Ÿæœªåˆå§‹åŒ–',
                    'mode': mode,
                    'status': 'error'
                }

            # æ‰§è¡ŒæŸ¥è¯¢
            response = await self.rag.aquery(question, param=QueryParam(mode=mode))

            return {
                'question': question,
                'answer': response,
                'mode': mode,
                'status': 'success'
            }

        except Exception as e:
            logging.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'question': question,
                'answer': f'æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}',
                'mode': mode,
                'status': 'error'
            }

    def print_query_result(self, result: Dict[str, Any]):
        """æ‰“å°æŸ¥è¯¢ç»“æœ"""
        print("=" * 80)
        print(f"é—®é¢˜: {result['question']}")
        print(f"æ¨¡å¼: {result['mode']}")
        print(f"\nå›ç­”:\n{result['answer']}")
        print("=" * 80)

    async def interactive_session(self):
        """äº¤äº’å¼ä¼šè¯"""
        print("\nğŸ‰ LightRAG äº¤äº’å¼é—®ç­”ç³»ç»Ÿ")
        print("æ”¯æŒæ¨¡å¼: hybrid, local, global, naive")
        print("è¾“å…¥ 'mode:æ¨¡å¼ é—®é¢˜' æŒ‡å®šæŸ¥è¯¢æ¨¡å¼")
        print("è¾“å…¥ 'quit' é€€å‡º, 'reindex' é‡æ–°ç´¢å¼•\n")

        while True:
            try:
                user_input = input("è¯·è¾“å…¥é—®é¢˜: ").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("å†è§ï¼")
                    break

                if user_input.lower() == 'reindex':
                    print("é‡æ–°ç´¢å¼•ä¸­...")
                    result = await self.process_knowledge_base()
                    if result['success']:
                        print(f"é‡æ–°ç´¢å¼•å®Œæˆ: {result['success_count']}/{result['total_files']} ä¸ªæ–‡æ¡£")
                    else:
                        print(f"é‡æ–°ç´¢å¼•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    continue

                # è§£ææŸ¥è¯¢æ¨¡å¼
                mode = "hybrid"
                question = user_input

                if user_input.startswith("mode:"):
                    parts = user_input.split(" ", 1)
                    if len(parts) == 2:
                        mode_part = parts[0].replace("mode:", "")
                        if mode_part in ["hybrid", "local", "global", "naive"]:
                            mode = mode_part
                            question = parts[1]

                # æ‰§è¡ŒæŸ¥è¯¢
                result = await self.query(question, mode=mode)
                self.print_query_result(result)

            except KeyboardInterrupt:
                print("\nç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                logging.error(f"å¤„ç†å‡ºé”™: {e}")
                print(f"å¤„ç†å‡ºé”™: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL.upper()),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(LOG_FILE, encoding='utf-8')
        ]
    )

    print("LightRAG çŸ¥è¯†æ£€ç´¢é—®ç­”ç³»ç»Ÿ")

    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system = LightRAGSystem(
            working_dir=LIGHTRAG_WORKING_DIR,
            knowledge_base_path=KNOWLEDGE_BASE_PATH
        )

        # æ£€æŸ¥æœåŠ¡å¹¶åˆå§‹åŒ–
        if not rag_system.check_services():
            print("æœåŠ¡æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿ç›¸å…³æœåŠ¡æ­£å¸¸è¿è¡Œ")
            return

        if not await rag_system.initialize():
            print("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return

        # å¤„ç†çŸ¥è¯†åº“æ–‡æ¡£
        result = await rag_system.process_knowledge_base()
        if result['success']:
            print(f"çŸ¥è¯†åº“å¤„ç†å®Œæˆ: {result['success_count']}/{result['total_files']} ä¸ªæ–‡æ¡£")
        else:
            print(f"çŸ¥è¯†åº“å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        # å¼€å§‹äº¤äº’å¼ä¼šè¯
        await rag_system.interactive_session()

    except Exception as e:
        logging.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        print(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
    finally:
        # æ¸…ç†å­˜å‚¨
        if rag_system and rag_system.rag:
            try:
                await rag_system.rag.finalize_storages()
                logging.info("å­˜å‚¨å·²æ¸…ç†")
            except Exception as e:
                logging.warning(f"å­˜å‚¨æ¸…ç†å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())