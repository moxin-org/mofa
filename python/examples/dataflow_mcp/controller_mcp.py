import asyncio
import os
import json

from dotenv import load_dotenv
from fastmcp import Client
from openai import OpenAI

# é…ç½® LLM ç”¨äºè§„åˆ’å·¥å…·è°ƒç”¨
load_dotenv('.env.secret')
os.environ['OPENAI_API_KEY'] = os.getenv('LLM_API_KEY')
planner_llm = OpenAI()


async def main():
    # ç¡®ä¿å®¢æˆ·ç«¯è¿æ¥çš„URLä¸æœåŠ¡å™¨ç«¯å®é™…ç›‘å¬çš„URLä¸€è‡´ï¼Œæ³¨æ„æœ«å°¾çš„æ–œæ 
    # æ ¹æ®æ‚¨ä¹‹å‰æœåŠ¡å™¨çš„å¯åŠ¨æ—¥å¿—ï¼šğŸ”— Server URL: http://127.0.0.1:9000/mcp/
    client = Client("http://127.0.0.1:9000/mcp/")

    async with client:
        # è·å– available toolsï¼Œä¾› LLM é€‰æ‹©
        tools = await client.list_tools()

        # --- è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°ä» FastMCP æœåŠ¡å™¨è·å–çš„åŸå§‹å·¥å…·åˆ—è¡¨ ---
        print("\n--- ä» FastMCP æœåŠ¡å™¨å‘ç°çš„åŸå§‹å·¥å…·ä¿¡æ¯ ---")
        if not tools:
            print("è­¦å‘Šï¼šæ²¡æœ‰å‘ç°ä»»ä½•å·¥å…·ã€‚è¯·ç¡®ä¿ deepsearch_mcp.py æœåŠ¡å™¨å·²æˆåŠŸå¯åŠ¨å¹¶æ³¨å†Œäº†å·¥å…·ã€‚")
        else:
            for i, t in enumerate(tools):
                print(f"å·¥å…· {i + 1}:")
                print(f"  åç§°: {getattr(t, 'name', 'N/A')}")
                print(f"  æè¿°: {getattr(t, 'description', 'N/A')}")
                print(f"  è¾“å…¥ Schema: {getattr(t, 'inputSchema', 'N/A')}")
                # ä½¿ç”¨ hasattr å’Œ getattr æ¥å®‰å…¨è®¿é—®å±æ€§ï¼Œé˜²æ­¢å±æ€§ä¸å­˜åœ¨å¯¼è‡´é”™è¯¯
        print("--------------------------------------------------\n")

        tool_defs = []
        for t in tools:
            # éªŒè¯å·¥å…·åç§°
            tool_name = getattr(t, 'name', None)
            if not tool_name:
                print(f"è­¦å‘Šï¼šå‘ç°ä¸€ä¸ªæ²¡æœ‰åç§°çš„å·¥å…·ã€‚è·³è¿‡æ­¤å·¥å…·ã€‚åŸå§‹æ•°æ®: {t}")
                continue

            # éªŒè¯å·¥å…·æè¿° (æè¿°å¯ä»¥ä¸ºç©ºï¼Œä½†ç¡®ä¿å®ƒæ˜¯å­—ç¬¦ä¸²)
            tool_description = getattr(t, 'description', "")
            if not isinstance(tool_description, str):
                tool_description = str(tool_description)  # å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²

            # éªŒè¯å’Œæ ¼å¼åŒ–è¾“å…¥ Schema
            # OpenAI çš„ parameters å­—æ®µè¦æ±‚æ˜¯ JSON Schema å¯¹è±¡
            # å³ä½¿å·¥å…·æ²¡æœ‰å‚æ•°ï¼Œä¹Ÿåº”è¯¥æ˜¯ {"type": "object", "properties": {}}
            tool_parameters = getattr(t, 'inputSchema', None)

            if not tool_parameters:
                # å¦‚æœ inputSchema ä¸ºç©ºæˆ– Noneï¼Œåˆ™æä¾›ä¸€ä¸ªæœ€å°çš„æœ‰æ•ˆ JSON Schema
                tool_parameters = {"type": "object", "properties": {}}
            elif isinstance(tool_parameters, str):
                # å¦‚æœ inputSchema æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•å°†å…¶è§£æä¸º JSON
                try:
                    tool_parameters = json.loads(tool_parameters)
                except json.JSONDecodeError:
                    print(
                        f"è­¦å‘Šï¼šå·¥å…· '{tool_name}' çš„ inputSchema ä¸æ˜¯æœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²ï¼Œå°è¯•è·³è¿‡æ­¤å·¥å…·ã€‚åŸå§‹Schema: {t.inputSchema}")
                    continue
            elif not isinstance(tool_parameters, dict):
                # å¦‚æœ inputSchema æ—¢ä¸æ˜¯ None ä¹Ÿä¸æ˜¯å­—ç¬¦ä¸²ä¹Ÿä¸æ˜¯å­—å…¸ï¼Œåˆ™æœ‰é—®é¢˜
                print(
                    f"è­¦å‘Šï¼šå·¥å…· '{tool_name}' çš„ inputSchema ç±»å‹ä¸æ­£ç¡®ï¼Œå°è¯•è·³è¿‡æ­¤å·¥å…·ã€‚ç±»å‹: {type(tool_parameters)}, åŸå§‹Schema: {t.inputSchema}")
                continue

            # ç¡®ä¿ tool_parameters æ˜¯ä¸€ä¸ªç¬¦åˆ OpenAI è¦æ±‚çš„æœ€å° JSON Schema ç»“æ„
            if "type" not in tool_parameters:
                tool_parameters["type"] = "object"
            if "properties" not in tool_parameters:
                tool_parameters["properties"] = {}

            # æ„å»ºç¬¦åˆ OpenAI API è¦æ±‚çš„å·¥å…·å®šä¹‰
            tool_def = {
                "type": "function",
                "function": {
                    "name": tool_name,
                    "description": tool_description,
                    "parameters": tool_parameters,
                }
            }
            tool_defs.append(tool_def)

        # --- è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ ¼å¼åŒ–åçš„å·¥å…·å®šä¹‰ï¼Œè¿™å°†å‘é€ç»™ OpenAI ---
        print("\n--- æ ¼å¼åŒ–åå‘é€ç»™ OpenAI API çš„å·¥å…·å®šä¹‰ ---")
        if not tool_defs:
            print("é”™è¯¯ï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„å·¥å…·å®šä¹‰ã€‚æ— æ³•ç»§ç»­ LLM è§„åˆ’ã€‚")
            return  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå·¥å…·ï¼Œç›´æ¥é€€å‡º
        else:
            print(json.dumps(tool_defs, indent=2, ensure_ascii=False))  # ensure_ascii=False ä»¥ä¾¿æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
        print("----------------------------------------------------\n")

        user_query = """æˆ‘æƒ³åˆ›å»ºä¸€ä¸ª mofaçš„ node  ä»»åŠ¡æ˜¯ä¸‹é¢çš„å†…å®¹ï¼š æŸ¥è¯¢è‹±æ–‡å•è¯é‡Šä¹‰ I want to create an agent to query the meaning of a certain word def define_word(word): response = requests.get(f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}") if response.ok: definition = response.json()[0]["meanings"][0]["definitions"][0]["definition"] return f"{word}: {definition}" return "æœªæ‰¾åˆ°é‡Šä¹‰" print(define_word("serendipity"))"""

        # LLM è§„åˆ’å·¥å…·è°ƒç”¨
        # åªæœ‰åœ¨æœ‰å¯ç”¨å·¥å…·æ—¶æ‰ä¼ é€’ functions å‚æ•°
        if tool_defs:
            plan_resp = planner_llm.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ agentï¼Œèƒ½å†³å®šæ˜¯å¦è°ƒç”¨ deepsearch å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚"},
                    # æœ€å¥½å°† tool_defs ä½œä¸º JSON å­—ç¬¦ä¸²åµŒå…¥åˆ° assistant æ¶ˆæ¯ä¸­ï¼Œä»¥ä¾¿ LLM æ›´å¥½åœ°ç†è§£
                    {"role": "assistant", "content": f"Available tools: {json.dumps(tool_defs, ensure_ascii=False)}"},
                    {"role": "user", "content": user_query}
                ],
                tools=tool_defs,  # ä¼ é€’æ ¼å¼åŒ–åçš„å·¥å…·å®šä¹‰åˆ—è¡¨
                tool_choice="auto"
            )
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·ï¼Œç›´æ¥è®© LLM å›ç­”ï¼Œä¸è¿›è¡Œå·¥å…·è°ƒç”¨
            print("æ²¡æœ‰å¯ç”¨çš„å·¥å…·ï¼ŒLLM å°†ç›´æ¥å›ç­”ã€‚")
            plan_resp = planner_llm.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ agentã€‚"},
                    {"role": "user", "content": user_query}
                ]
            )
            print("LLM æœ€ç»ˆå›å¤ (æ— å·¥å…·):", plan_resp.choices[0].message.content)
            return

        # æ£€æŸ¥ LLM æ˜¯å¦ç¡®å®è¿›è¡Œäº†å·¥å…·è°ƒç”¨
        if not plan_resp.choices[0].message.tool_calls:
            print("LLM å†³å®šä¸è°ƒç”¨ä»»ä½•å·¥å…·ã€‚æœ€ç»ˆå›å¤:")
            print(plan_resp.choices[0].message.content)
            return

        func_call = plan_resp.choices[0].message.tool_calls[0]
        args = json.loads(func_call.function.arguments)

        print(f"\n--- LLM å†³å®šè°ƒç”¨å·¥å…·ï¼š{func_call.function.name}ï¼Œå‚æ•°ï¼š{args} ---")

        # è°ƒç”¨ deepsearchï¼ˆæˆ–å…¶ä»–å·¥å…·ï¼‰
        tool_result = await client.call_tool(
            func_call.function.name,
            args
        )
        print(f"\n--- å·¥å…·è°ƒç”¨ç»“æœï¼š{tool_result.data} ---")

        # LLM ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
        final_resp = planner_llm.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "åŸºäºå·¥å…·çš„æœç´¢ç»“æœï¼Œç»™å‡ºåˆ†ææ¯”è¾ƒã€‚"},
                {"role": "user", "content": user_query},
                # ç¡®ä¿ tool_result.data æ˜¯ JSON å­—ç¬¦ä¸²
                {"role": "assistant", "content": json.dumps(tool_result.data or {}, ensure_ascii=False)}
            ]
        )
        print("\næœ€ç»ˆç»“æœï¼š", final_resp.choices[0].message.content)


if __name__ == "__main__":
    asyncio.run(main())