import json
import random
import time
import os
import uuid
from typing import List, Dict, Optional
import openai
import numpy as np
from dotenv import load_dotenv
from playwright.sync_api import expect

from mofa.agent_build.base.base_agent import MofaAgent, run_agent
from mofa.kernel.tools.web_search import search_web_with_serper
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env.secret')
openai.api_key = os.getenv("LLM_API_KEY")
openai.api_base = os.getenv("LLM_BASE_URL")
DEFAULT_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "gpt-4o")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")

# é…ç½®é¡¹
MAX_ARTICLES = 50  # æœ€å¤§å¤„ç†æ–‡ç« æ•°
DEFAULT_STREAM_DELAY = 0.1  # æµå¼å»¶è¿Ÿ


class LLMClient:
    def __init__(self, file_path: str = '.env.secret', model_name: str = 'gpt-4o'):
        self.model_name = model_name
        load_dotenv('.env.secret')
        if os.getenv('LLM_API_KEY') is not None:
            os.environ['OPENAI_API_KEY'] = os.getenv('LLM_API_KEY')
        if os.getenv('LLM_BASE_URL', None) is None:
            client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])
        else:
            client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], base_url=os.getenv('LLM_BASE_URL'))
        self.client = client

    def generate_response(self, messages: List[Dict], max_tokens: int = 3200, stream: bool = True):
        """
        Generate responses in a streaming fashion
        """
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=max_tokens,
            stream=stream  # Enable streaming responses
        )

        for chunk in response:
            # Each chunk contains a portion of content
            content = chunk.choices[0].delta.content
            yield "" if content is None else content  # Ensure we always yield a string


class ArticleRef:
    def __init__(self, title: str, url: str, snippet: str, source: str, relevance: float = 1.0):
        self.title = title
        self.url = url
        self.snippet = snippet
        self.source = source
        self.relevance = relevance

    def dict(self) -> dict:
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "source": self.source,
            "relevance": self.relevance
        }


class ArticleProcessor:
    """
    ç®€åŒ–ç‰ˆæ–‡ç« å¤„ç†å™¨ï¼š
    1. å°† serper è¿”å›çš„ç»“æœè½¬æ¢ä¸º ArticleRef å¯¹è±¡
    2. åŸºäº URL å»é‡
    3. æ ¹æ® snippet é•¿åº¦é™åºæ’åº
    """

    def __init__(self, serper_results: List[Dict]):
        self.articles = [
            ArticleRef(
                title=article.get("name", ""),
                url=article.get("url", ""),
                snippet=article.get("snippet", ""),
                source=article.get("name", ""),
                relevance=len(article.get("snippet", ""))
            )
            for article in serper_results
        ]

    def process(self) -> List[ArticleRef]:
        unique = self._remove_duplicates(self.articles)
        filtered = self._filter_quality(unique)
        return filtered

    def _remove_duplicates(self, articles: List[ArticleRef]) -> List[ArticleRef]:
        seen_urls = set()
        unique_articles = []
        for article in articles:
            if article.url not in seen_urls:
                seen_urls.add(article.url)
                unique_articles.append(article)
        return unique_articles

    def _filter_quality(self, articles: List[ArticleRef]) -> List[ArticleRef]:
        return sorted(articles, key=lambda a: a.relevance, reverse=True)


class ResearchGenerator:
    """ç ”ç©¶ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ LLMClient æ ¹æ®æ–‡ç« å’Œæ€è€ƒé˜¶æ®µç”Ÿæˆè¾“å‡ºï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""

    def __init__(self, articles: List[ArticleRef], llm_client: Optional[LLMClient] = None, max_output: int = 20):
        self.articles = articles
        self.max_output = max_output
        self.used_articles = set()
        self.llm_client = llm_client or LLMClient()
        self.thinking_stages = [
            {
                "name": "context_extraction",
                "description": "ğŸ“ Extract key context from the articles by isolating the most informative snippets.",
                "article_selector": lambda articles: articles[:min(3, len(articles))]
            },
            {
                "name": "intent_analysis",
                "description": "ğŸ” Analyze the core user intent by examining the extracted context.",
                "article_selector": lambda articles: articles[:min(3, len(articles))]
            },
            {
                "name": "source_eval",
                "description": "ğŸ“Š Evaluate source credibility by ranking articles based on trustworthiness.",
                "article_selector": lambda articles: self._select_by_metric(articles, 'source')
            },
            {
                "name": "contradiction_check",
                "description": "âš ï¸ Check for information consistency by cross-referencing articles.",
                "article_selector": lambda articles: articles[::max(1, len(articles) // 3)]
            },
            {
                "name": "synthesis",
                "description": "ğŸ§  Synthesize insights by integrating the validated information.",
                "article_selector": lambda articles: articles
            }
        ]
        self.phase_prompts = [
            "Based on the current research, what are the most notable trends and insights?",
            "Key findings from the articles reviewed include...",
            "What are the gaps in the current studies and how might they be addressed?",
        ]

    def _select_by_metric(self, articles: List[ArticleRef], metric: str) -> List[ArticleRef]:
        if metric == 'source':
            preferred_sources = ["journal", "report", "web"]
            filtered = [a for a in articles if a.source.lower() in preferred_sources]
            filtered.sort(key=lambda a: (preferred_sources.index(a.source.lower()), -a.relevance))
            return filtered
        return articles

    def _llm_think(self, stage_description: str, selected_articles: List[ArticleRef], stage_id: int, substep_id: int,
                   stage_name: str = None):
        context = " ".join([a.snippet for a in selected_articles])
        messages = [
            {"role": "system", "content": stage_description},
            {"role": "user", "content": f"Analyze the following context: {context}"}
        ]
        if stage_name is None:
            stage_name = stage_description
        for sub_stage_id, chunk in enumerate(self.llm_client.generate_response(messages)):
            yield {
                "type": "thinking",
                "content": chunk,
                "articles": [a.dict() for a in selected_articles],
                "metadata": {"stage": stage_name},
                'id': f"{stage_id}-{sub_stage_id}"  # å½“å‰é˜¶æ®µçš„ ID å’Œå­æ­¥éª¤ ID
            }

    def _think_summary(self, data: str, stage_id: int = 6):
        summary_prompt = (
            "Context: Below is a summary of key insights and observations derived from various stages of thinking:\n"
            f"{data}\n\n"
            "Objective: Your task is to generate a comprehensive summary that encapsulates the key findings from the above thinking stages.\n\n"
            "Structure: The summary should be clear, organized, and easy to follow. It should have distinct sections addressing the following points:\n"
            "1. Main insights and key takeaways from the thinking process.\n"
            "2. Any contradictions or inconsistencies identified and how they were handled.\n"
            "3. Overall understanding of the subject and any conclusions drawn from the thinking process.\n\n"
            "Tactics: Break the summary into coherent paragraphs, ensuring that each section is easily identifiable.\n"
            "Use bullet points, numbered lists, or any other suitable structure to improve clarity and readability.\n\n"
            "Action: Once the summary is generated, make sure it highlights the most important insights and findings that will guide the next steps.\n\n"
            "Result: The expected result should be a well-organized, informative summary that consolidates the most crucial points from the thinking process."
        )
        messages = [
            {"role": "system", "content": "You are an expert summarizer"},
            {"role": "user", "content": summary_prompt}
        ]
        for sub_stage_id, chunk in enumerate(self.llm_client.generate_response(messages)):
            yield {
                "type": "thinking",
                "content": chunk,
                "articles": [],
                "metadata": {"stage": "thinking summary"},
                'id': f"{stage_id}-{sub_stage_id}"  # å½“å‰é˜¶æ®µçš„ ID å’Œå­æ­¥éª¤ ID
            }

    def _llm_generate_content(self, prompt: str, related_articles: List[ArticleRef], stage_id: int, substep_id: int,
                              stage: str = None, type_content: str = None):
        context = " ".join([a.snippet for a in related_articles])
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Generate content based on the context: {context}"}
        ]

        if stage is None:
            stage = prompt
        if type_content is None:
            type_content = 'content'
        for sub_stage_id, chunk in enumerate(self.llm_client.generate_response(messages)):
            yield {
                "type": type_content,
                "content": chunk,
                "articles": [a.dict() for a in related_articles],
                "metadata": {"stage": stage},
                'id': f"{stage_id}-{sub_stage_id}",  # å½“å‰é˜¶æ®µçš„ ID å’Œå­æ­¥éª¤ ID

            }
        if type_content == 'completion':
            yield {
                "type": type_content,
                "content": '',
                "articles": [],
                "metadata": {"stage": stage},
                'id': f"{stage_id}",  # å½“å‰é˜¶æ®µçš„ ID å’Œå­æ­¥éª¤ ID
                'end': 'yes'

            }

    def generate_stream(self, user_query: str = ''):
        content_outputs = []  # ç”¨äºæ”¶é›†å„é˜¶æ®µç”Ÿæˆçš„å†…å®¹
        substep_id = 0  # åˆå§‹å­æ­¥éª¤ID
        think_outputs = []
        # ç”Ÿæˆå„ä¸ªæ€è€ƒé˜¶æ®µçš„è¾“å‡ºï¼ˆåŒæ­¥æ–¹å¼ï¼‰
        for stage_id, stage in enumerate(self.thinking_stages):
            selected = stage["article_selector"](self.articles)
            context_articles = selected[:min(3, len(selected))]
            for article in context_articles:
                self.used_articles.add(article.url)
            for chunk in self._llm_think(stage["description"], context_articles, stage_id, substep_id, stage['name']):
                content = chunk.get('content', '')
                if content is not None:  # Ensure we only append string values
                    think_outputs.append(content)
                yield chunk
            substep_id += 1  # Increment substep
            stage_id += 1  # Increment stage ID

        # Filter out any None values before joining
        think_outputs = [output for output in think_outputs if output is not None]
        think_summary = ''
        for chunk in self._think_summary(''.join(think_outputs)):
            think_summary += chunk.get('content', '')
            yield chunk

        num_articles = len(self.articles)
        chunk_size = max(1, num_articles // len(self.phase_prompts))

        # ä¿®å¤ï¼šæŒ‰æ¯ä¸ªå°éƒ¨åˆ†è¿”å›å¹¶åŒ…æ‹¬æ­£ç¡®çš„stageå’Œid
        for i, prompt in enumerate(self.phase_prompts):
            start = i * chunk_size
            end = (i + 1) * chunk_size if i < len(self.phase_prompts) - 1 else num_articles
            related = self.articles[start:end]
            for article in related:
                self.used_articles.add(article.url)
            llm_content = ''
            for chunk in self._llm_generate_content(
                    prompt + '\n This is the content of the previous Think Summary' + think_summary + f'\n This is question {user_query}',
                    related, stage_id, substep_id, stage=prompt.replace('...', '')):
                llm_content += chunk.get('content')
                yield chunk
            content_outputs.append(llm_content)
            time.sleep(DEFAULT_STREAM_DELAY)
            substep_id += 1  # å­æ­¥éª¤é€’å¢
            stage_id += 1  # å¢åŠ é˜¶æ®µID

        # æœ€åè¿›è¡Œæœ€ç»ˆç»¼åˆï¼šè°ƒç”¨ LLM å¯¹æ‰€æœ‰å†…å®¹è¾“å‡ºåšæ€»ç»“
        final_context = "\n".join(content_outputs)
        final_prompt = (
                "Context: The aggregated content below contains insights gathered from multiple analysis stagesâ€”namely, context extraction, intent analysis, "
                "source evaluation, contradiction check, and preliminary synthesisâ€”pertaining to the subject under investigation.\n\n"
                "Objective: Generate a comprehensive final synthesis that integrates all key insights from the provided content.\n\n"
                "Strategy:\n"
                "  - Carefully review the aggregated content below.\n"
                "  - Identify and recap the main points from each analysis stage.\n"
                "  - Produce a detailed, structured summary using bullet points or numbered sections for clarity.\n\n"
                "Tactics:\n"
                "  1. Recap the extracted context and highlight the core user intent regarding the topic.\n"
                "  2. Evaluate the credibility and reliability of the sources mentioned.\n"
                "  3. Summarize any contradictions or discrepancies identified and explain how they were addressed.\n"
                "  4. Synthesize all the information into a coherent overview that emphasizes the key findings, technological innovations, "
                "and overall value proposition of the subject matter.\n\n"
                "Audience: The final synthesis should be detailed, clear, and suitable for industry professionals and researchers seeking an in-depth analysis.\n\n"
                "Action: Based on the aggregated content provided below, generate a final, structured synthesis that covers all the critical insights and details.\n\n"
                "Aggregated Content:\n" + final_context + "\n\nFinal Synthesis: "
                                                          "Aggregated Thinking Data: \n" + think_summary + "\n\n"

        )
        final_synthesis = self._llm_generate_content(final_prompt, self.articles, 0, 0, 'Complete the report',
                                                     'completion')
        yield from final_synthesis


@run_agent
def run(agent: MofaAgent):
    user_query = agent.receive_parameter('user_query')
    load_dotenv('.env.secret')
    print('user_query:', user_query)

    raw_articles = search_web_with_serper(query=user_query, subscription_key=os.getenv("SERPER_API_KEY"))
    print("Serper search returned:")
    print(json.dumps(raw_articles, indent=2))

    processor = ArticleProcessor(raw_articles)
    processed_articles = processor.process()  # åŒæ­¥è°ƒç”¨
    selected_articles = processed_articles[:20]

    llm_client = LLMClient(model_name=os.getenv("LLM_MODEL_NAME", DEFAULT_MODEL_NAME))

    generator = ResearchGenerator(articles=selected_articles, llm_client=llm_client)

    print("\n--- Generating output ---\n")
    results = []
    for chunk in generator.generate_stream(user_query=user_query):
        results.append(json.dumps(chunk, indent=2))
        agent.send_output(agent_output_name='deep_inquire_result', agent_result=json.dumps(chunk, indent=2))


def main():
    agent = MofaAgent(agent_name='DeepInquire')
    run(agent=agent)


if __name__ == "__main__":
    main()
